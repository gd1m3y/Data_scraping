{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapping.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFfJlrPgiwy9cd2A8D45HQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gd1m3y/Data_scraping/blob/main/scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMUTEBujDHbK"
      },
      "source": [
        "#Scrapping text data\r\n",
        "[first function](https://colab.research.google.com/drive/1N2auJ9eEg2HdfKn8lSSLFTLSj55m4810#scrollTo=VEmaRLOV2BqA&line=33&uniqifier=1)\r\n",
        "[second Function]()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJNA4D2pnzj9"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "from collections import defaultdict\r\n",
        "import pandas as pd"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mx583ecqnd3"
      },
      "source": [
        "test_url = \"https://muckrack.com/beat/bizfin\"\r\n",
        "test_url = \"https://muckrack.com/media-outlet/wsj\"\r\n",
        "test_url = \"https://muckrack.com/media-outlet/washpost\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI99fcvXr7Vz"
      },
      "source": [
        "r = requests.get(test_url)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al_os8k_r-7P"
      },
      "source": [
        "html_response = r.text"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyLhm8X7r_4Y"
      },
      "source": [
        "soup = BeautifulSoup(html_response,'html.parser')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80dUAQ_sLyu"
      },
      "source": [
        "text = soup.find('div',{'class':\"row bottom-sm\" })"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t0284QFsncw"
      },
      "source": [
        "# text"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LN_KM9ouKsL"
      },
      "source": [
        "links = text.find_all('a')\r\n",
        "link_dict={}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqxRXcQmuq4S",
        "outputId": "473aa97b-f5fc-473e-9c35-192945d5ed5a"
      },
      "source": [
        "for link in links:\r\n",
        " anchor_text = link.get_text()\r\n",
        " link_dict[anchor_text] = link['href']\r\n",
        "print(link_dict)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Abelson, Jenn': '/jenn-abelson', 'Aberman, Jonathan': '/jaberman', 'Abernathy, Gary': '/abernathygary', 'Ables, Kelsey': '/kelsey-ables', 'Abutaleb, Yasmeen': '/yasmeen-abutaleb', 'Achenbach, Joel': '/joel-achenbach', 'Adam, Karla': '/karla-adam', 'Agawu, Emefa Addo': '/emefa-addo-agawu', 'Ahmed, Naema': '/naema-ahmed', 'Ajaka, Nadine': '/nadine-ajaka', 'Akhtar, Monica': '/monica-akhtar', 'Akkad, Reem': '/reem-akkad', 'Albergotti, Reed': '/reedalbergotti', 'Albright, Mary Beth': '/mary-beth-albright', 'Alcantara, Chris': '/chris-alcantara', 'Aldag, Jason': '/jason-aldag', 'Alemany, Jacqueline': '/jacqueline-alemany', 'Alexander, Keith L.': '/keith-alexander', 'Alfaro, Mariana': '/mariana-alfaro', 'Ali, Anjuman': '/anjuman-ali-1', 'Allan, M. Carrie': '/carrie_the_red', 'Allen, Danielle': '/danielle-allen', 'Allen, Scott': '/scott-allen', 'Althoff, Eric': '/eric-althoff', 'Alvarez, Joshua': '/joshua-alvarez', 'Alvarez, Lizette': '/lizettenyt', 'Amenabar, Teddy': '/teddy-amenabar', 'Amur, Jennifer': '/jennifer-amur', 'Anderson, Emily': '/emily-anderson-6', 'Anderson, Nick': '/wpnick', 'Andrews, Travis': '/travis-andrews', 'Andrews-Dyer, Helena': '/helena-andrews', 'Aratani, Lori': '/loriara', 'Argetsinger, Amy': '/amy-argetsinger', 'Armao, Jo-Ann': '/jo-ann-armao-1', 'Armus, Teo': '/teoarmus', 'Arthur, Nicole': '/nicolearthur', 'Attiah, Karen': '/karen-attiah', 'Aydıntaşbaş, Aslı': '/asliaydintasbas', 'Babb, Kent': '/kentbabb', 'Bade, Rachael': '/rachaelmbade', 'Bahrampour, Tara': '/tara-bahrampour', 'Bai, Matt': '/mattbai', 'Bailey, Holly': '/hollybdc', 'Balingit, Moriah': '/moriah-balingit', 'Balko, Radley': '/radleybalko', 'Balz, Dan': '/danbalz', 'Baran, Jonathan': '/jmhall_', 'Barber, Greg': '/greg-barber-1', 'Barkley, Lillian': '/lbarkle2', 'Barnes, Bart': '/bart-barnes', 'Barnes, Robert': '/robert-barnes', 'Baron, Martin': '/postbaron', 'Barr, Cameron': '/cameron-barr', 'Barr, Jeremy': '/jeremy-barr', 'Barrett, Devlin': '/devlin-barrett', 'Barrios, Jennifer': '/jennifer-barrios', 'Barron, Christina': '/christina-barron', 'Beachum, Lateshia': '/lateshia-beachum', 'Bearak, Max': '/max-bearak', 'Beck, Luisa': '/luisa-beck', 'Bell, Braden': '/braden-bell', 'Bella, Tim': '/tim-bella', 'Bendavid, Naftali': '/naftalibendavid', 'Bennett, Dalton': '/dalton-bennett', 'Benning, Victoria': '/victoria-benning', 'Berger, Miriam': '/miriam-berger', 'Berkowitz, Bonnie': '/bonnie-berkowitz', 'Berman, Jae': '/jae-berman', 'Berman, Mark': '/markberman', 'Bernstein, Adam': '/adam-bernstein', 'Bernstein, Lenny': '/lenny-bernstein', 'Betancourt, David': '/adcfanboy', 'Bethea, April': '/aprilbethea', 'Bever, Lindsey': '/lindseybever', 'Bhattarai, Abha': '/abhabhattarai', 'Bieler, Desmond': '/des-bieler', 'Binder, Sarah': '/sarah-binder', 'Birnbaum, Michael': '/michaelbirnbaum', 'Birnholz, Evan': '/evan-birnholz', 'Blackistone, Kevin B.': '/kevin-b-blackistone', 'Blake, Aaron': '/aaronblakewp', 'Blanco, Adrián': '/adrian-blanco', 'Boburg, Shawn': '/shawn-boburg', 'Bogage, Jacob': '/jacob-bogage', 'Bolek, Rachael': '/rachael-bolek', 'Bone, Julie': '/julie-bone', 'Bonesteel, Matt': '/matt-bonesteel', 'Bonne, Jon': '/jbonne', 'Bonos, Lisa': '/lisa-bonos', 'Boodman, Sandra': '/sandra-boodman', 'Boorstein, Michelle': '/michelle-boorstein', 'Boot, Max': '/max-boot-1', 'Booth, William': '/boothwilliam', 'Boren, Cindy': '/cindyboren', 'Boswell, Thomas': '/thomas-boswell', 'Botsford, Jabin': '/jabin-botsford', 'Bowen, Fred': '/fred-bowen', 'Bradley, Mark': '/mark-bradley-1', 'Branigin, William': '/william-branigin'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y_KkvsRuuDn"
      },
      "source": [
        "#exploring a single page\r\n",
        "base_url ='https://muckrack.com'\r\n",
        "author_url = base_url+list(link_dict.values())[3]\r\n",
        "# author_url_new = \"https://muckrack.com/fredrik-arnold-rydlun\"\r\n",
        "author_req = requests.get(author_url)\r\n",
        "author_text = author_req.text\r\n",
        "author_soup = BeautifulSoup(author_text,'html.parser')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyUpNJeSzVbV"
      },
      "source": [
        "text = author_soup.find('div',{'class':\"profile-section-content\" })"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kNslaC9H-61r",
        "outputId": "65995d81-b5ae-4245-d866-6836fd677965"
      },
      "source": [
        "location = author_soup.findAll('div',{'class':\"person-details-item person-details-location\"})[0].findAll('div')[0].get_text()\r\n",
        "location = re.sub('[\\n\\t\\s]*',\"\",location)\r\n",
        "location"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Washington,D.C.,UnitedStates'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h__iZg9_AHjr",
        "outputId": "8b1c8072-6e6d-4458-8da1-95ae4e10c5e9"
      },
      "source": [
        "name = text.findAll('h1')[0].get_text().split()\r\n",
        "name = \" \".join(x for x in name)\r\n",
        "name"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kelsey Ables'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nuZhL4049sod",
        "outputId": "562b9307-73a8-4d03-c62d-9e20da02dcd4"
      },
      "source": [
        "job_title = author_soup.findAll('div',{'class':\"person-details-item person-details-title\"})[0].findAll('div')[0].get_text().split()\r\n",
        "job_title = \" \".join(x for x in job_title)\r\n",
        "job_title"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Arts Writer — The Washington Post'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fCbSBRiuApJX",
        "outputId": "a29dd5a1-1c54-4d45-9ac8-0f59120cb6bc"
      },
      "source": [
        "interests = author_soup.findAll('div',{'class':\"person-details-item person-details-beats\"})[0].findAll('a')\r\n",
        "interests = \",\".join(x.get_text() for x in interests)\r\n",
        "interests"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Arts and Entertainment,Metro D.C.,U.S.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "YlUpgmBdFr1b",
        "outputId": "47d5cf47-a4b5-414c-ff71-f61e9c2c3b16"
      },
      "source": [
        "verified = author_soup.find('small',{'class':\"profile-verified text-uppercase color-green\"}).get_text()\r\n",
        "verified = re.sub('[\\s\\n]*',\"\",verified)\r\n",
        "verified"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-5ed323de6ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mverified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthor_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"profile-verified text-uppercase color-green\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mverified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[\\s\\n]*'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mverified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM3GCARQFTc1",
        "outputId": "36d8270f-612e-4149-de52-d98c7abc3ef6"
      },
      "source": [
        "as_seen_in = author_soup.find('div',{'class':\"profile-details-item\"})\r\n",
        "as_seen_in = {x.get_text().strip():base_url+author_soup.find('div',{'class':\"profile-details-item\"}).findAll('a')[0]['href'] for x in as_seen_in.findAll('a')}\r\n",
        "outlets = {}\r\n",
        "# if as_seen_in != None:\r\n",
        "#   outlets[as_seen_in] = base_url+author_soup.find('div',{'class':\"profile-details-item\"}).findAll('a')[0]['href']\r\n",
        "# outlets\r\n",
        "as_seen_in"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Artsy': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'CNN': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'Columbia Journalism Review': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'Daily Herald (Chicago, IL)': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'Laredo Morning Times': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'Seattle Times': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'Spokesman-Review': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'Stars and Stripes': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'Terra': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'The Diplomat Magazine': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'The Washington Post': 'https://muckrack.com/media-outlet/washpost',\n",
              " 'more': 'https://muckrack.com/media-outlet/washpost'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "dyoiGKgoKoyb",
        "outputId": "ed25d99e-56d6-4f6e-e996-f402bcd51a12"
      },
      "source": [
        "about = author_soup.find('div',{'class':'mr-font-size-lg mr-font-family-2 top-sm'})\r\n",
        "about = about.get_text().strip()\r\n",
        "about"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'arts @washingtonpost丨former elephant reporter丨gone museuming丨previously: @artsy, @princetoninasia, @cjr, @artvoice丨columbia ‘18 丨kelsey.ables@washpost.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7Do6CZn_LM9p",
        "outputId": "fff18a66-8525-49d0-f0d4-76c2cb99b9f2"
      },
      "source": [
        "profile_picture_link = author_soup.find('div',{'class':'mr-profile-avatar-container col-xs-3'})\r\n",
        "profile_picture_link = profile_picture_link.findAll('img')[0]['src']\r\n",
        "profile_picture_link"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://pbs.twimg.com/profile_images/1233616723187728384/giIjlsxx_400x400.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKXOb9t3NP9J"
      },
      "source": [
        "#checking for online profiles"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7HnVNnWnNQD9",
        "outputId": "df623475-7582-4b2a-e9e9-3f08a5008749"
      },
      "source": [
        "social = author_soup.findAll('div',{'class':'profile-section-social'})[0]\r\n",
        "social = \" , \".join(x['href'] for x in social.findAll('a'))\r\n",
        "social"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://t.co/e2Nux7edUE , http://twitter.com/ables_kelsey , https://www.linkedin.com/in/kelsey-ables-314774113/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybVo_qN6NQIS"
      },
      "source": [
        "# final api fuction"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ean5ftNQPm"
      },
      "source": [
        "test_url = \"https://muckrack.com/beat/bizfin\"\r\n",
        "final_frame = defaultdict(list)\r\n",
        "while(next_link):\r\n",
        "  test_req = requests.get(test_url)\r\n",
        "  test_html = test_req.text\r\n",
        "  test_soup = BeautifulSoup(test_html,'html.parser')\r\n",
        "  next_link = test_soup.findAll(\"ul\",{'class':'pager top-none bottom-none'})\r\n",
        "  try:\r\n",
        "    next_link = [x['href'] for x in next_link[0].find_all('a') if x.get_text().strip() == 'Next' ]\r\n",
        "  except:\r\n",
        "    next_link = None\r\n",
        "    break\r\n",
        "  link = base_url+next_link[0]\r\n",
        "  test_url = link\r\n",
        "  link_text = test_soup.find('div',{'class':\"row bottom-sm\" })\r\n",
        "  links = link_text.find_all('a')\r\n",
        "  link_dict={}\r\n",
        "  for link in links:\r\n",
        "    anchor_text = link.get_text()\r\n",
        "    link_dict[anchor_text] = link['href']  \r\n",
        "  \r\n",
        "  author_lis = list(link_dict.values())\r\n",
        "  \r\n",
        "  for i in range(len(author_lis)):\r\n",
        "    #exploring a single page\r\n",
        "    base_url ='https://muckrack.com'\r\n",
        "    author_url = base_url+author_lis[i]\r\n",
        "    # author_url_new = \"https://muckrack.com/fredrik-arnold-rydlun\"\r\n",
        "    author_req = requests.get(author_url)\r\n",
        "    author_text = author_req.text\r\n",
        "    author_soup = BeautifulSoup(author_text,'html.parser')\r\n",
        "    try:\r\n",
        "      location = author_soup.findAll('div',{'class':\"person-details-item person-details-location\"})\r\n",
        "      location = re.sub('[\\n\\t\\s]*',\"\",location[0].findAll('div')[0].get_text()) if location else \" \"\r\n",
        "      \r\n",
        "      name = author_soup.findAll('h1')\r\n",
        "      name = \" \".join(x for x in name[0].get_text().split()) if name != [] else \" \"\r\n",
        "      \r\n",
        "      job_title = author_soup.findAll('div',{'class':\"person-details-item person-details-title\"})\r\n",
        "      job_title = \" \".join(x for x in job_title[0].findAll('div')[0].get_text().split()) if job_title else \" \" \r\n",
        "      \r\n",
        "      interests = author_soup.findAll('div',{'class':\"person-details-item person-details-beats\"})[0].findAll('a')\r\n",
        "      interests = \",\".join(x.get_text() for x in interests) if interests else \" \"\r\n",
        "      \r\n",
        "      verified = author_soup.find('small',{'class':\"profile-verified text-uppercase color-green\"})\r\n",
        "      verified = re.sub('[\\s\\n]*',\"\",verified.get_text()) if verified else 'Not Verified' \r\n",
        "      \r\n",
        "      as_seen_in = author_soup.find('div',{'class':\"profile-details-item\"})\r\n",
        "      as_seen_in = {x.get_text().strip():base_url+x['href'] for x in as_seen_in.findAll('a')} if as_seen_in else None\r\n",
        "\r\n",
        "      \r\n",
        "      about = author_soup.find('div',{'class':'mr-font-size-lg mr-font-family-2 top-sm'})\r\n",
        "      about = about.get_text().strip() if about else \" \"\r\n",
        "      \r\n",
        "      # profile_picture_link = author_soup.find('div',{'class':'mr-profile-avatar-container col-xs-3'})\r\n",
        "      # profile_picture_link = profile_picture_link.findAll('img')[0]['src']\r\n",
        "      \r\n",
        "      main_social = author_soup.findAll('div',{'class':'profile-section-social'})\r\n",
        "      main_social = \",\".join(x['href'] for x in main_social[0].findAll('a')) if main_social else None\r\n",
        "      other_social = []\r\n",
        "      facebook = None\r\n",
        "      linkdin = None\r\n",
        "      twitter = None\r\n",
        "      if main_social == None:\r\n",
        "        other_social = main_social\r\n",
        "        facebook = None\r\n",
        "        linkdin = None\r\n",
        "        twitter = None\r\n",
        "      else:\r\n",
        "        for website in main_social.split(','):\r\n",
        "          if 'facebook' in website:\r\n",
        "            facebook = website\r\n",
        "            \r\n",
        "          elif 'linkedin' in website:\r\n",
        "            linkdin = website\r\n",
        "          elif 'twitter' in website:\r\n",
        "            twitter = website\r\n",
        "            \r\n",
        "          else:\r\n",
        "            other_social.append(website)\r\n",
        "      \r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "      # print(name , job_title , interests , verified , as_seen_in , about , profile_picture_link ,social)\r\n",
        "    except Exception as e:\r\n",
        "      print('some error',e)\r\n",
        "      continue\r\n",
        "\r\n",
        "\r\n",
        "    final_frame['author_name'].append(name)\r\n",
        "    final_frame['job_title'].append(job_title)\r\n",
        "    final_frame['location'].append(interests)\r\n",
        "    final_frame['verified'].append(verified)\r\n",
        "    final_frame['outlets'].append(as_seen_in)\r\n",
        "    final_frame['about'].append(about)\r\n",
        "    # final_frame['profile_picture_link'].append(profile_picture_link)\r\n",
        "    final_frame['other_social'].append(other_social)\r\n",
        "    final_frame['facebook'].append(facebook)\r\n",
        "    final_frame['linkedin'].append(linkdin)\r\n",
        "    final_frame['twitter'].append(twitter)\r\n",
        "  print(final_frame.keys())\r\n",
        "  #capture href of outlets very very imp\r\n",
        "  #sep facebook /mukrak profile scrapping "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfB2APRPUPDU",
        "outputId": "98bd7035-7de5-41d0-e487-7be2590823b9"
      },
      "source": [
        "import pandas as pd\r\n",
        "frame = pd.DataFrame(final_frame)\r\n",
        "frame.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26064 entries, 0 to 26063\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   author_name   26064 non-null  object\n",
            " 1   job_title     26064 non-null  object\n",
            " 2   location      26064 non-null  object\n",
            " 3   verified      26064 non-null  object\n",
            " 4   outlets       25629 non-null  object\n",
            " 5   about         26064 non-null  object\n",
            " 6   other_social  24931 non-null  object\n",
            " 7   facebook      10997 non-null  object\n",
            " 8   linkedin      22762 non-null  object\n",
            "dtypes: object(9)\n",
            "memory usage: 1.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV_7RRD1SW3F",
        "outputId": "f4bba3de-45f7-446f-9d33-19d0a47f18c8"
      },
      "source": [
        "'facebook' in 'facebook.com'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAWfFnvtKC-P",
        "outputId": "85740639-2ff6-42c9-a11f-ef689d5a7d1e"
      },
      "source": [
        "frame.outlets.iloc[4]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Fit Small Business': 'https://muckrack.com/media-outlet/fitsmallbusiness',\n",
              " 'Hillsboro Banner': 'https://muckrack.com/media-outlet/hillsborobanner'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxRsVrMhNQT9"
      },
      "source": [
        "frame.to_csv('authors_final_big_data.csv')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIsg_csmWzX2"
      },
      "source": [
        "test_req = requests.get(test_url)\r\n",
        "test_html = test_req.text\r\n",
        "test_soup = BeautifulSoup(test_html,'html.parser')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGFrAt4eNQXp"
      },
      "source": [
        "test_url = \"https://muckrack.com/beat/bizfin\"\r\n",
        "next_link = True\r\n",
        "while(next_link):\r\n",
        "  test_req = requests.get(test_url)\r\n",
        "  test_html = test_req.text\r\n",
        "  test_soup = BeautifulSoup(test_html,'html.parser')\r\n",
        "  next_link = test_soup.findAll(\"ul\",{'class':'pager top-none bottom-none'})\r\n",
        "  try:\r\n",
        "    next_link = [x['href'] for x in next_link[0].find_all('a') if x.get_text().strip() == 'Next' ]\r\n",
        "  except:\r\n",
        "    next_link == None\r\n",
        "    break\r\n",
        "  link = base_url+next_link[0]\r\n",
        "  test_url = link\r\n",
        "  link_text = test_soup.find('div',{'class':\"row bottom-sm\" })\r\n",
        "  links = link_text.find_all('a')\r\n",
        "  link_dict={}\r\n",
        "  for link in links:\r\n",
        "    anchor_text = link.get_text()\r\n",
        "    link_dict[anchor_text] = link['href']\r\n",
        "  print(link_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjxGC8a917ld"
      },
      "source": [
        "\r\n",
        "## Implementing final functions\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEmaRLOV2BqA"
      },
      "source": [
        "def Get_author_list(outlet):\r\n",
        "  '''\r\n",
        "  returns dictionary with\r\n",
        "  keys: author names\r\n",
        "  values: Profile links\r\n",
        "\r\n",
        "  requires requests,beautifulsoup Module\r\n",
        "  '''\r\n",
        "  author_dict = {}\r\n",
        "  base_url = \"https://muckrack.com\"\r\n",
        "  test_url =base_url+\"/media-outlet/\"+outlet\r\n",
        "  author_lis = []\r\n",
        "  link_dict={}\r\n",
        "  # print(test_url)\r\n",
        "  while (True):\r\n",
        "    test_req = requests.get(test_url)\r\n",
        "    test_html = test_req.text\r\n",
        "    test_soup = BeautifulSoup(test_html,'html.parser')\r\n",
        "    next_link = test_soup.findAll(\"ul\",{'class':'pager top-none bottom-none'})\r\n",
        "    # print(next_link)\r\n",
        "    try:\r\n",
        "      next_link = [x['href'] for x in next_link[0].find_all('a') if x.get_text().strip() == 'Next' ]\r\n",
        "    except:\r\n",
        "      next_link = None\r\n",
        "      break\r\n",
        "    link = base_url+next_link[0]\r\n",
        "    test_url = link\r\n",
        "    # print(link)\r\n",
        "    link_text = test_soup.find('div',{'class':\"row bottom-sm\" })\r\n",
        "    links = link_text.find_all('a')\r\n",
        "    \r\n",
        "    for link in links:\r\n",
        "      anchor_text = link.get_text()\r\n",
        "      link_dict[anchor_text] = base_url+link['href']  \r\n",
        "      \r\n",
        "    \r\n",
        "  return link_dict\r\n",
        "\r\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Qku_9tDFsh"
      },
      "source": [
        "second\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HOlrBvZ4Mbj"
      },
      "source": [
        "x = Get_author_list('wsj')"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnCFdTJL4gUa"
      },
      "source": [
        "def Get_author_data(author_link_list):\r\n",
        "  '''\r\n",
        "  Takes in a input of list and returns the author details in data frame format \r\n",
        "\r\n",
        "\r\n",
        "  input: List of the links of author profiles \r\n",
        "\r\n",
        "  output: Dataframe containing all the author details \r\n",
        "  \r\n",
        "  requires defaultdict and pandas to be imported\r\n",
        "  '''\r\n",
        "  final_frame = defaultdict(list)\r\n",
        "  for author_link in author_link_list:\r\n",
        "    author_req = requests.get(author_link)\r\n",
        "    author_text = author_req.text\r\n",
        "    author_soup = BeautifulSoup(author_text,'html.parser')\r\n",
        "    try:\r\n",
        "      location = author_soup.findAll('div',{'class':\"person-details-item person-details-location\"})\r\n",
        "      location = re.sub('[\\n\\t\\s]*',\"\",location[0].findAll('div')[0].get_text()) if location else \" \"\r\n",
        "      \r\n",
        "      name = author_soup.findAll('h1')\r\n",
        "      name = \" \".join(x for x in name[0].get_text().split()) if name != [] else \" \"\r\n",
        "      \r\n",
        "      job_title = author_soup.findAll('div',{'class':\"person-details-item person-details-title\"})\r\n",
        "      job_title = \" \".join(x for x in job_title[0].findAll('div')[0].get_text().split()) if job_title else \" \" \r\n",
        "      \r\n",
        "      interests = author_soup.findAll('div',{'class':\"person-details-item person-details-beats\"})\r\n",
        "      try:\r\n",
        "        interests = interests[0].findAll('a')\r\n",
        "        interests = \",\".join(x.get_text() for x in interests) if interests else \" \"\r\n",
        "      except:\r\n",
        "        interests  = None\r\n",
        "      \r\n",
        "      verified = author_soup.find('small',{'class':\"profile-verified text-uppercase color-green\"})\r\n",
        "      verified = re.sub('[\\s\\n]*',\"\",verified.get_text()) if verified else 'Not Verified' \r\n",
        "      \r\n",
        "      as_seen_in = author_soup.find('div',{'class':\"profile-details-item\"})\r\n",
        "      as_seen_in = {x.get_text().strip():base_url+x['href'] for x in as_seen_in.findAll('a')} if as_seen_in else None\r\n",
        "\r\n",
        "      \r\n",
        "      about = author_soup.find('div',{'class':'mr-font-size-lg mr-font-family-2 top-sm'})\r\n",
        "      about = about.get_text().strip() if about else \" \"\r\n",
        "      \r\n",
        "      # profile_picture_link = author_soup.find('div',{'class':'mr-profile-avatar-container col-xs-3'})\r\n",
        "      # profile_picture_link = profile_picture_link.findAll('img')[0]['src']\r\n",
        "      \r\n",
        "      main_social = author_soup.findAll('div',{'class':'profile-section-social'})\r\n",
        "      main_social = \",\".join(x['href'] for x in main_social[0].findAll('a')) if main_social else None\r\n",
        "      other_social = []\r\n",
        "      facebook = None\r\n",
        "      linkdin = None\r\n",
        "      twitter = None\r\n",
        "      if main_social == None:\r\n",
        "        other_social = main_social\r\n",
        "        facebook = None\r\n",
        "        linkdin = None\r\n",
        "        twitter = None\r\n",
        "      else:\r\n",
        "        for website in main_social.split(','):\r\n",
        "          if 'facebook' in website:\r\n",
        "            facebook = website\r\n",
        "            \r\n",
        "          elif 'linkedin' in website:\r\n",
        "            linkdin = website\r\n",
        "          elif 'twitter' in website:\r\n",
        "            twitter = website\r\n",
        "            \r\n",
        "          else:\r\n",
        "            other_social.append(website)\r\n",
        "      \r\n",
        "      \r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "        # print(name , job_title , interests , verified , as_seen_in , about , profile_picture_link ,social)\r\n",
        "    except Exception as e:\r\n",
        "      print(name)\r\n",
        "      print('some error',e)\r\n",
        "      \r\n",
        "    final_frame['author_name'].append(name)\r\n",
        "    final_frame['job_title'].append(job_title)\r\n",
        "    final_frame['location'].append(interests)\r\n",
        "    final_frame['verified'].append(verified)\r\n",
        "    final_frame['outlets'].append(as_seen_in)\r\n",
        "    final_frame['about'].append(about)\r\n",
        "    # final_frame['profile_picture_link'].append(profile_picture_link)\r\n",
        "    final_frame['other_social'].append(other_social)\r\n",
        "    final_frame['facebook'].append(facebook)\r\n",
        "    final_frame['linkedin'].append(linkdin)\r\n",
        "    final_frame['twitter'].append(twitter)\r\n",
        "  final_frame = pd.DataFrame(final_frame)\r\n",
        "  return final_frame"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYrQ1keA-cQL"
      },
      "source": [
        "test = list(x.values())[:30]\r\n",
        "Get_author_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcpuolLo_XiO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}